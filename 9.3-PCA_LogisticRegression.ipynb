{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA with Logistic Regression\n",
    "\n",
    "The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\n",
    "\n",
    "\n",
    "The MNIST database of handwritten digits is available on the following website: MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Load the DataÂ¶\n",
    "\n",
    "Download the data from the link:\n",
    "\n",
    "https://github.com/amplab/datascience-sp14/blob/master/lab7/mldata/mnist-original.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mnist = scipy.io.loadmat('C:\\\\Users\\\\Owner\\\\Downloads\\\\College\\\\CPSC\\\\sem_04\\\\mnist-original.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 70000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the images\n",
    "mnist['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 9., 9., 9.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data = np.transpose(mnist['data'])\n",
    "mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [9.],\n",
       "       [9.],\n",
       "       [9.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the labels\n",
    "mnist_label = np.transpose(mnist['label'])\n",
    "mnist_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)   (70000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_data.shape,' ',mnist_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 159, 253,\n",
       "       159,  50,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238,\n",
       "       252, 252, 252, 237,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  54,\n",
       "       227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,\n",
       "        60, 224, 252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 163, 252, 252, 252, 253, 252, 252,  96, 189, 253, 167,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  51, 238, 253, 253, 190, 114, 253, 228,  47,  79, 255,\n",
       "       168,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  48, 238, 252, 252, 179,  12,  75, 121,  21,   0,\n",
       "         0, 253, 243,  50,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  38, 165, 253, 233, 208,  84,   0,   0,   0,\n",
       "         0,   0,   0, 253, 252, 165,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   7, 178, 252, 240,  71,  19,  28,   0,\n",
       "         0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  57, 252, 252,  63,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 198, 253, 190,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0, 255, 253, 196,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  76, 246, 252,\n",
       "       112,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 253, 252,\n",
       "       148,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85,\n",
       "       252, 230,  25,   0,   0,   0,   0,   0,   0,   0,   0,   7, 135,\n",
       "       253, 186,  12,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  85, 252, 223,   0,   0,   0,   0,   0,   0,   0,   0,   7,\n",
       "       131, 252, 225,  71,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  85, 252, 145,   0,   0,   0,   0,   0,   0,   0,\n",
       "        48, 165, 252, 173,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  86, 253, 225,   0,   0,   0,   0,   0,\n",
       "         0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  85, 252, 249, 146,  48,  29,\n",
       "        85, 178, 225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 252, 252, 252,\n",
       "       229, 215, 252, 252, 252, 196, 130,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  28, 199,\n",
       "       252, 252, 253, 252, 252, 233, 145,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,  25, 128, 252, 253, 252, 141,  37,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c5e7bf5c18>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#for val in mnist_data[5]:\n",
    " #   pixel = val.reshape(28,28)\n",
    "  #  plt.matshow(pixel)\n",
    "    \n",
    "plt.matshow(mnist_data[68000].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size: what proportion of original data is used for test set\n",
    "train_img, test_img, train_lbl, test_lbl = train_test_split(\n",
    "    mnist_data, mnist_label, test_size=(1/7.0), random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 1)\n",
      "(10000, 784)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_img.shape)\n",
    "print(train_lbl.shape)\n",
    "print(test_img.shape)\n",
    "print(test_lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the Data\n",
    "\n",
    "Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(train_img)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "train_img = scaler.transform(train_img)\n",
    "test_img = scaler.transform(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA to Speed up Classification Algorithms (Logistic Regression)\n",
    "\n",
    "#### Step 0: Import and use PCA. After PCA you can apply a learning algorithm of your choice to the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Make an instance of the Model\n",
    "\n",
    "pca = PCA(.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit PCA on training set. Note: you are fitting PCA on the training set only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=0.95, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(train_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the mapping (transform) to both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = pca.transform(train_img)\n",
    "test_img = pca.transform(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import the model you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# parameters not specified are set to their defaults\n",
    "# default solver is incredibly slow thats why we change it\n",
    "# solver = 'lbfgs'\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Training the model on the data, storing the information learned from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Owner\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(train_img, train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.19629663e+00, -2.47009935e+00,  3.78176536e+00,  1.15564634e+00,\n",
       "        6.04803105e+00,  6.33886886e-01,  2.20555116e+00, -3.44946247e+00,\n",
       "        1.53824059e+00, -2.31504359e+00, -2.05854994e+00,  2.09543799e+00,\n",
       "        1.05084455e-01, -3.90567115e+00, -1.56880567e+00,  1.08064936e+00,\n",
       "       -4.15254795e-01,  1.32854801e+00, -6.47158784e-01, -4.48541530e-01,\n",
       "       -1.95222216e+00, -6.41021778e-01,  1.07359302e+00, -7.83768345e-01,\n",
       "       -5.60844952e-01,  1.11019197e+00, -1.75083103e+00,  6.60730898e-01,\n",
       "        1.15653856e+00,  4.54276513e-01,  4.97826840e-01, -3.70694521e-01,\n",
       "       -9.73318134e-01, -1.80974515e+00,  1.76989339e-01,  8.97012813e-01,\n",
       "       -8.76820063e-01, -4.69191712e-01, -5.97901589e-01, -1.23109405e-01,\n",
       "       -7.13657046e-01,  5.30181011e-01, -1.73105380e-01,  2.51795402e-01,\n",
       "       -3.22432709e-01,  2.04992082e-01,  4.09875301e-02,  1.25655790e-01,\n",
       "       -5.55903756e-02,  8.46557343e-01,  4.87791417e-01, -1.25966892e-01,\n",
       "        4.39460998e-01, -3.19295998e-01, -1.01611173e+00,  8.69186255e-01,\n",
       "        4.30576899e-01,  4.07744709e-01, -1.00191121e-01,  9.63289597e-02,\n",
       "        4.72924784e-02, -2.80568752e-01, -3.19184907e-02, -1.70042287e-01,\n",
       "       -2.02158277e-01, -5.18858674e-01, -6.30860296e-01, -8.98383998e-01,\n",
       "        7.49560245e-01, -1.50214256e-01,  6.75208457e-02,  3.55525464e-01,\n",
       "       -1.53591888e-01, -5.25415923e-01, -9.20423733e-01,  2.22218478e-02,\n",
       "       -1.92561419e-01,  3.97637855e-01,  8.65730465e-01,  1.06274760e+00,\n",
       "       -2.61138682e-02,  1.00611427e-01,  1.08710605e-01, -2.08089689e-01,\n",
       "        9.82692704e-02,  1.49372635e-01, -2.38580262e-01,  2.11436222e-01,\n",
       "        4.69114123e-01, -8.00088037e-01, -5.75838722e-01,  6.82333407e-02,\n",
       "       -8.99593862e-01,  3.27652916e-01,  2.44069836e-01,  2.65509088e-01,\n",
       "        1.50970700e-01,  5.21835241e-01,  2.10340764e-01, -4.72129931e-01,\n",
       "       -2.60802367e-01, -3.33161954e-01,  1.02757767e-01,  7.22340856e-01,\n",
       "       -4.68371371e-01, -3.68022777e-01, -4.89278291e-01,  1.29515552e-01,\n",
       "        8.88668657e-02, -6.30453444e-01,  4.86881621e-01,  9.28482177e-02,\n",
       "        5.67505196e-01, -3.52555933e-01, -1.06299379e-02, -8.83369176e-02,\n",
       "       -1.48288827e-01,  6.40357193e-01,  5.23647199e-01, -5.89695398e-01,\n",
       "        7.16779933e-01, -6.50651339e-01,  5.60852291e-02,  8.08255078e-01,\n",
       "       -1.06684373e+00, -2.49941855e-01, -1.86871461e-01, -5.03066169e-01,\n",
       "       -1.60439781e-01, -2.62528911e-02,  9.07691021e-01, -2.79761178e-01,\n",
       "        5.58270531e-01, -1.47042888e-01, -5.57164579e-01, -4.59076639e-01,\n",
       "       -2.24088572e-01, -5.92270587e-01, -8.37541106e-02, -7.87485175e-01,\n",
       "       -1.83162441e-01, -2.50943823e-01,  6.05308713e-01,  2.92743026e-01,\n",
       "        4.81588671e-01, -1.58987009e-01,  1.30431481e-01, -6.46566068e-01,\n",
       "        9.34473098e-01, -6.21115848e-01, -2.00200310e-01, -5.18390663e-02,\n",
       "       -9.07302322e-02, -2.17499696e-01, -6.51756804e-02,  3.51935350e-01,\n",
       "        9.95074933e-03,  4.36969910e-01,  4.73083178e-01, -1.99590271e-01,\n",
       "        4.24472716e-01,  7.07742403e-02, -2.48913712e-01,  2.61338339e-01,\n",
       "        1.89923141e-01,  3.43630490e-02, -2.70963702e-01, -1.18734029e-02,\n",
       "       -2.93371521e-01,  2.57565146e-02, -2.14462001e-02, -1.52238087e-01,\n",
       "        1.06947423e+00, -2.60268085e-01,  6.51659053e-01, -7.82640374e-02,\n",
       "        1.34720720e-01, -4.58607756e-03,  6.11274862e-01,  3.61482574e-01,\n",
       "       -4.09333050e-01,  2.38797910e-03,  1.11357260e-01,  1.45486264e-01,\n",
       "       -5.22437779e-01, -3.58236062e-01, -1.95675140e-01, -5.61676795e-01,\n",
       "        6.16517969e-01, -2.33211699e-01, -1.05048572e-01, -3.28265530e-01,\n",
       "        2.28760981e-01,  3.03628371e-01, -9.66963070e-02,  6.59017957e-01,\n",
       "       -2.08946744e-01,  1.95312718e-01,  8.41386862e-02, -2.21846352e-02,\n",
       "        6.47060126e-02,  7.14810007e-01,  3.91346389e-02,  2.05372183e-02,\n",
       "       -5.30582184e-01,  4.37092304e-01, -2.19295263e-01, -1.73595273e-02,\n",
       "       -8.87985631e-02,  5.75336690e-01,  3.97546690e-01, -3.24577832e-01,\n",
       "       -2.99140030e-01, -6.66213890e-01, -3.31864992e-01,  5.17921365e-01,\n",
       "       -1.65783714e-01,  1.05597492e-01,  3.31061751e-01, -7.63593486e-01,\n",
       "        3.69041623e-01, -4.76391069e-01,  5.60513485e-01, -1.38832537e-01,\n",
       "       -8.00849779e-01,  2.14485195e-02, -5.18826466e-01, -7.00200597e-01,\n",
       "       -1.92312335e-01, -4.75280333e-01,  4.15467576e-02,  3.00704687e-01,\n",
       "        5.30245195e-01,  2.41271317e-01, -2.59660923e-01, -4.15979682e-01,\n",
       "       -2.11690504e-01, -1.77636860e-02, -6.26142643e-01,  2.16183165e-01,\n",
       "        5.07077314e-01, -4.90754320e-01, -1.52162318e-01, -7.01184434e-02,\n",
       "        1.17635263e-01, -1.62286554e-01,  3.37780750e-01,  1.86532101e-01,\n",
       "        1.11970674e-01, -6.07937292e-02,  7.07553541e-02,  1.63561961e-01,\n",
       "        1.23827803e-01, -3.17728740e-01, -1.38617217e-02,  1.79596308e-02,\n",
       "        1.18657436e-01,  6.38600289e-02,  1.37563246e-01,  3.67776311e-01,\n",
       "       -1.84858039e-01,  3.08692612e-01,  1.42033634e-01,  4.79850790e-02,\n",
       "        9.09868056e-02,  3.57992835e-01, -4.63291350e-01,  8.89058430e-01,\n",
       "        7.23723786e-01, -8.12269868e-04,  2.17692650e-02,  1.79601416e-01,\n",
       "       -5.92024015e-02, -4.38948097e-01, -2.84878987e-01, -7.13528112e-01,\n",
       "        2.92080895e-01,  1.37563026e-01,  1.44873217e-01,  2.61041347e-01,\n",
       "        1.76256373e-01,  1.67426154e-01, -6.99547697e-01,  7.71145096e-02,\n",
       "       -1.84120094e-01,  3.40007524e-01,  1.21611544e-01,  1.44017783e-01,\n",
       "        2.01933069e-01,  2.62479996e-02, -3.32922822e-01,  7.06688501e-02,\n",
       "        1.00746261e-01,  1.57790627e-02, -1.00415021e-01, -1.94943006e-01,\n",
       "        1.57222487e-01,  2.03083716e-01, -4.43641385e-01,  2.58143384e-01,\n",
       "        1.65330674e-01, -5.74881546e-01, -4.16709377e-01, -2.80056685e-01,\n",
       "        4.63355262e-02, -1.14359874e-01,  3.30784931e-01, -3.93420338e-01,\n",
       "        1.79498409e-01,  1.27442671e-01, -1.04565675e-01,  1.09249269e-01,\n",
       "        1.68117810e-01, -1.96669904e-01,  3.35690369e-01,  3.72490514e-01,\n",
       "       -1.10701503e-01, -9.95234805e-02,  2.75202753e-01, -2.34791739e-01,\n",
       "       -4.42910670e-01,  3.85270798e-01, -2.12279024e-01, -1.50462898e-01,\n",
       "        1.79384971e-01,  3.57296072e-01,  3.18198292e-01,  1.97976922e-02,\n",
       "       -2.68918170e-01,  1.87443946e-01])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.19629663e+00, -2.47009935e+00,  3.78176536e+00,\n",
       "         1.15564634e+00,  6.04803105e+00,  6.33886886e-01,\n",
       "         2.20555116e+00, -3.44946247e+00,  1.53824059e+00,\n",
       "        -2.31504359e+00, -2.05854994e+00,  2.09543799e+00,\n",
       "         1.05084455e-01, -3.90567115e+00, -1.56880567e+00,\n",
       "         1.08064936e+00, -4.15254795e-01,  1.32854801e+00,\n",
       "        -6.47158784e-01, -4.48541530e-01, -1.95222216e+00,\n",
       "        -6.41021778e-01,  1.07359302e+00, -7.83768345e-01,\n",
       "        -5.60844952e-01,  1.11019197e+00, -1.75083103e+00,\n",
       "         6.60730898e-01,  1.15653856e+00,  4.54276513e-01,\n",
       "         4.97826840e-01, -3.70694521e-01, -9.73318134e-01,\n",
       "        -1.80974515e+00,  1.76989339e-01,  8.97012813e-01,\n",
       "        -8.76820063e-01, -4.69191712e-01, -5.97901589e-01,\n",
       "        -1.23109405e-01, -7.13657046e-01,  5.30181011e-01,\n",
       "        -1.73105380e-01,  2.51795402e-01, -3.22432709e-01,\n",
       "         2.04992082e-01,  4.09875301e-02,  1.25655790e-01,\n",
       "        -5.55903756e-02,  8.46557343e-01,  4.87791417e-01,\n",
       "        -1.25966892e-01,  4.39460998e-01, -3.19295998e-01,\n",
       "        -1.01611173e+00,  8.69186255e-01,  4.30576899e-01,\n",
       "         4.07744709e-01, -1.00191121e-01,  9.63289597e-02,\n",
       "         4.72924784e-02, -2.80568752e-01, -3.19184907e-02,\n",
       "        -1.70042287e-01, -2.02158277e-01, -5.18858674e-01,\n",
       "        -6.30860296e-01, -8.98383998e-01,  7.49560245e-01,\n",
       "        -1.50214256e-01,  6.75208457e-02,  3.55525464e-01,\n",
       "        -1.53591888e-01, -5.25415923e-01, -9.20423733e-01,\n",
       "         2.22218478e-02, -1.92561419e-01,  3.97637855e-01,\n",
       "         8.65730465e-01,  1.06274760e+00, -2.61138682e-02,\n",
       "         1.00611427e-01,  1.08710605e-01, -2.08089689e-01,\n",
       "         9.82692704e-02,  1.49372635e-01, -2.38580262e-01,\n",
       "         2.11436222e-01,  4.69114123e-01, -8.00088037e-01,\n",
       "        -5.75838722e-01,  6.82333407e-02, -8.99593862e-01,\n",
       "         3.27652916e-01,  2.44069836e-01,  2.65509088e-01,\n",
       "         1.50970700e-01,  5.21835241e-01,  2.10340764e-01,\n",
       "        -4.72129931e-01, -2.60802367e-01, -3.33161954e-01,\n",
       "         1.02757767e-01,  7.22340856e-01, -4.68371371e-01,\n",
       "        -3.68022777e-01, -4.89278291e-01,  1.29515552e-01,\n",
       "         8.88668657e-02, -6.30453444e-01,  4.86881621e-01,\n",
       "         9.28482177e-02,  5.67505196e-01, -3.52555933e-01,\n",
       "        -1.06299379e-02, -8.83369176e-02, -1.48288827e-01,\n",
       "         6.40357193e-01,  5.23647199e-01, -5.89695398e-01,\n",
       "         7.16779933e-01, -6.50651339e-01,  5.60852291e-02,\n",
       "         8.08255078e-01, -1.06684373e+00, -2.49941855e-01,\n",
       "        -1.86871461e-01, -5.03066169e-01, -1.60439781e-01,\n",
       "        -2.62528911e-02,  9.07691021e-01, -2.79761178e-01,\n",
       "         5.58270531e-01, -1.47042888e-01, -5.57164579e-01,\n",
       "        -4.59076639e-01, -2.24088572e-01, -5.92270587e-01,\n",
       "        -8.37541106e-02, -7.87485175e-01, -1.83162441e-01,\n",
       "        -2.50943823e-01,  6.05308713e-01,  2.92743026e-01,\n",
       "         4.81588671e-01, -1.58987009e-01,  1.30431481e-01,\n",
       "        -6.46566068e-01,  9.34473098e-01, -6.21115848e-01,\n",
       "        -2.00200310e-01, -5.18390663e-02, -9.07302322e-02,\n",
       "        -2.17499696e-01, -6.51756804e-02,  3.51935350e-01,\n",
       "         9.95074933e-03,  4.36969910e-01,  4.73083178e-01,\n",
       "        -1.99590271e-01,  4.24472716e-01,  7.07742403e-02,\n",
       "        -2.48913712e-01,  2.61338339e-01,  1.89923141e-01,\n",
       "         3.43630490e-02, -2.70963702e-01, -1.18734029e-02,\n",
       "        -2.93371521e-01,  2.57565146e-02, -2.14462001e-02,\n",
       "        -1.52238087e-01,  1.06947423e+00, -2.60268085e-01,\n",
       "         6.51659053e-01, -7.82640374e-02,  1.34720720e-01,\n",
       "        -4.58607756e-03,  6.11274862e-01,  3.61482574e-01,\n",
       "        -4.09333050e-01,  2.38797910e-03,  1.11357260e-01,\n",
       "         1.45486264e-01, -5.22437779e-01, -3.58236062e-01,\n",
       "        -1.95675140e-01, -5.61676795e-01,  6.16517969e-01,\n",
       "        -2.33211699e-01, -1.05048572e-01, -3.28265530e-01,\n",
       "         2.28760981e-01,  3.03628371e-01, -9.66963070e-02,\n",
       "         6.59017957e-01, -2.08946744e-01,  1.95312718e-01,\n",
       "         8.41386862e-02, -2.21846352e-02,  6.47060126e-02,\n",
       "         7.14810007e-01,  3.91346389e-02,  2.05372183e-02,\n",
       "        -5.30582184e-01,  4.37092304e-01, -2.19295263e-01,\n",
       "        -1.73595273e-02, -8.87985631e-02,  5.75336690e-01,\n",
       "         3.97546690e-01, -3.24577832e-01, -2.99140030e-01,\n",
       "        -6.66213890e-01, -3.31864992e-01,  5.17921365e-01,\n",
       "        -1.65783714e-01,  1.05597492e-01,  3.31061751e-01,\n",
       "        -7.63593486e-01,  3.69041623e-01, -4.76391069e-01,\n",
       "         5.60513485e-01, -1.38832537e-01, -8.00849779e-01,\n",
       "         2.14485195e-02, -5.18826466e-01, -7.00200597e-01,\n",
       "        -1.92312335e-01, -4.75280333e-01,  4.15467576e-02,\n",
       "         3.00704687e-01,  5.30245195e-01,  2.41271317e-01,\n",
       "        -2.59660923e-01, -4.15979682e-01, -2.11690504e-01,\n",
       "        -1.77636860e-02, -6.26142643e-01,  2.16183165e-01,\n",
       "         5.07077314e-01, -4.90754320e-01, -1.52162318e-01,\n",
       "        -7.01184434e-02,  1.17635263e-01, -1.62286554e-01,\n",
       "         3.37780750e-01,  1.86532101e-01,  1.11970674e-01,\n",
       "        -6.07937292e-02,  7.07553541e-02,  1.63561961e-01,\n",
       "         1.23827803e-01, -3.17728740e-01, -1.38617217e-02,\n",
       "         1.79596308e-02,  1.18657436e-01,  6.38600289e-02,\n",
       "         1.37563246e-01,  3.67776311e-01, -1.84858039e-01,\n",
       "         3.08692612e-01,  1.42033634e-01,  4.79850790e-02,\n",
       "         9.09868056e-02,  3.57992835e-01, -4.63291350e-01,\n",
       "         8.89058430e-01,  7.23723786e-01, -8.12269868e-04,\n",
       "         2.17692650e-02,  1.79601416e-01, -5.92024015e-02,\n",
       "        -4.38948097e-01, -2.84878987e-01, -7.13528112e-01,\n",
       "         2.92080895e-01,  1.37563026e-01,  1.44873217e-01,\n",
       "         2.61041347e-01,  1.76256373e-01,  1.67426154e-01,\n",
       "        -6.99547697e-01,  7.71145096e-02, -1.84120094e-01,\n",
       "         3.40007524e-01,  1.21611544e-01,  1.44017783e-01,\n",
       "         2.01933069e-01,  2.62479996e-02, -3.32922822e-01,\n",
       "         7.06688501e-02,  1.00746261e-01,  1.57790627e-02,\n",
       "        -1.00415021e-01, -1.94943006e-01,  1.57222487e-01,\n",
       "         2.03083716e-01, -4.43641385e-01,  2.58143384e-01,\n",
       "         1.65330674e-01, -5.74881546e-01, -4.16709377e-01,\n",
       "        -2.80056685e-01,  4.63355262e-02, -1.14359874e-01,\n",
       "         3.30784931e-01, -3.93420338e-01,  1.79498409e-01,\n",
       "         1.27442671e-01, -1.04565675e-01,  1.09249269e-01,\n",
       "         1.68117810e-01, -1.96669904e-01,  3.35690369e-01,\n",
       "         3.72490514e-01, -1.10701503e-01, -9.95234805e-02,\n",
       "         2.75202753e-01, -2.34791739e-01, -4.42910670e-01,\n",
       "         3.85270798e-01, -2.12279024e-01, -1.50462898e-01,\n",
       "         1.79384971e-01,  3.57296072e-01,  3.18198292e-01,\n",
       "         1.97976922e-02, -2.68918170e-01,  1.87443946e-01]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img[0].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 330)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img[4000].reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for One Observation (image)\n",
    "logisticRegr.predict(test_img[4000].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [9.],\n",
       "       [2.],\n",
       "       ...,\n",
       "       [6.],\n",
       "       [3.],\n",
       "       [6.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 9., 2., ..., 6., 3., 6.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for Multiple Observations (images) at Once\n",
    "logisticRegr.predict(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "score = logisticRegr.score(test_img, test_lbl)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
